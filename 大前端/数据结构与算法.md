---
title: "数据结构"
author: "凢凢"
date: "2019-09-02"
permalink: "arithmetic"
sidebar: "auto"
single: true
---

## 复杂度分析

数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码跑的更快，如何让代码运行得更快，如何让代码更省存储空间。
执行效率是算法一个非常重要的考量指标。

#### 为什么要复杂度分析？

执行效率的方法可以通过跑一遍代码，通过统计、监控、就能得到算法执行的时间和占用的内存大小，这种方法称之为事后统计法，但这种方法有很大的局限性。
比如测试结果依赖测试环境，测试结果受数据规模的影响很大。

#### 大 O 复杂度表示法

算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用“肉眼”得到一段代码的执行时间呢？

```js
function cal(n) {
  let sum = 0;
  for (let i = 1; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

从 CPU 的角度来看，这段代码的每一行都执行类似的操作：读数据-运算-写数据。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都是一样的，为 unit_time。

那么在这个假设之上，这段代码的总执行时间是多少呢？

```js
function cal(n) {
  let sum = 0;
  let i = 1;
  let j = 1;
  for (i; i <= n; ++i) {
    j = 1;
    for (j; j <= n; ++j) {
      sum = sum + i * j;
    }
  }
}
```

每个语句的执行时间为 unit*time，这段代码的总执行时间 T(n)是多少?
前面 3 个变量执行时间为 3 * unit*time,第一个 for 循环执行时间为 2n * unit_time,第二个 for 循环执行的时间为 2n^2 \* unit_time。这里的 2 是因为循环体总共有 2 行代码。

最后推导出的总执行时间为: `T(n) = (2n + 3 + 2n^2)*unit_time`

尽管我们不知道 unit_time 的具体指，但是通过两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间 T(n)与每行代码的执行次数 n 成正比。

公式： `T(n) = O(f(n));`

T(n)表示代码执行的时间，n 表示数据规模的大小，f(n)表示每行代码执行的次数总和，因为这是一个公式，所以用 f(n)来表示。公式中的 O，表示代码的执行时间 T(n)与 f(n)表达式成正比。

第一个例子 T(n) = O(2n+2) 与第二个例子 T(n) = O(2n^2+2n+3)。这就是大 O 时间复杂度表示法。
大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度，简称时间复杂度。

当 n 很大时，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长的趋势，所以都可以忽略不计。我们只需要记录一个最大量级就可以。

上面两段代码的用大 O 表示法来标记时间复杂度：
T(n) = O(n) T(n) = O(n^2)

#### 分析一段代码的时间复杂度

1. 只关注执行次数最多的一段代码 数字 1000 甚至更大不能表示数据规模，因此忽略
2. 加法法则，总复杂度等于量级最大的那段代码的复杂度
3. 乘法法则，嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

O(1)这是常量级时间复杂度的一种表示方法；并不是指只执行了一行代码，比如一段代码，即便有 3 行，它的时间复杂度也是 O(1)而不是 O(3);
只要代码的执行时间不随数据规模 n 的增长而增长，这样代码的时间复杂度我们都记做 O(1)。或者说只要代码不存在递归，或者循环，即便有上完行代码，其时间复杂度也为 O(1)。

O(logn)/O(nlogn)

对数阶

```js
let i = 1;
while (i <= n) {
  i = i * 2;
}
```

#### 空间复杂度

时间复杂度表示算法的执行时间与数据规模之间的增长关系。空间复杂度则表示算法的存储空间与数据规模的增长关系。

```js
function print(n) {
  let i = 0;
  let a = [];
  for (i; i < n; i++) {
    a[i] = i * i;
  }
  for (i = n - 1; i >= 0; --i) {
    print(a[i]);
  }
}
```

变量 i 与 n 无关因此忽略，变量 a 是一个数组，它的元素数量由 n 决定，所以这段代码的空间复杂度为 O(n)

复杂度用来分析执行效率与数据规模的增长关系。越高阶的算法，复杂度越高。

四个复杂度分析

- 最好情况时间复杂度
- 最坏情况时间复杂度
- 平均情况时间复杂度
- 均摊时间复杂度

#### 最好、最坏情况时间复杂度

```js
function find(arr, n, x) {
  let i = 0;
  let pos = -1;
  for (i; i < n; ++i) {
    if (arr[i] === x) pos = i;
  }
  return pos;
}

// 复杂度为 O(n),n 为数组长度
```

```js
function find(arr, n, x) {
  let i = 0;
  let pos = -1;
  for (i; i < n; ++i) {
    if (arr[i] === x) pos = i;
    break; // 优化
  }
  return pos;
}
```

优化后，最理想的情况下，查找的变量正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况的时间复杂度。
同理，最坏情况时间复杂度就是，在最糟糕的情况下，代码执行的时间复杂度。就是数组中没有查找到变量 x，需要把整个数组遍历一遍才行。

最好时间复杂度和最坏时间复杂度对应的都是极端情况下的代码复杂度，发送的概率其实并不大。为了更好的表示平均值情况下的复杂度，引入一个新的概念，平均时间复杂度。

在查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0~n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后在除以 n + 1,就可以得到需要遍历的元素个数的平均值。

计算过程中，我们假设在数组中与不在数组中的概率为 1/2。另外，要查找的数据出现在 0~n-1 这个 n 位置的概率也是一样的，为 1/n。所以，根据 概率乘法法则，要查找的数据出现在 0~n-1 中任意位置的概率就是 1 / 2n。

一段代码在不同的情况下，会产生不同的时间复杂度。

#### 均摊时间复杂度

```js
var arr = Array(n);
let count = 0;

function insert(val) {
  if (count === arr.length) {
    let sum = 0;
    for (let i = 0; i < arr.lenth; ++i) {
      sum = sum + arr[i];
    }
    arr[0] = sum;
    count = 1;
  }
  arr[coun] = val;
  ++count;
}
```

最理想的情况，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 O(1)。最坏情况下，数组中没有空闲位置了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n);
平均时间复杂度为 O(1);

## 数组

数组为什么要从 0 开始编号？

数组是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。

- 线性表：数据排成像线条一样的结构，线性表上的数据最多只有前和后两个方向。
- 非线性表：二叉树，堆，图等，之所以叫非线性，是因为数据之间并不是简单的前后关系。

连续的内存空间和相同类型的数据，正是因为这 2 个限制，它才有了 “随机访问”。但这两个限制也让数组的很多操作变得非常低效，比如想要在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。

```js
const arr = Array(10); // 计算机个数组arr分配了一块连续内存空间1000~1039,其中内存块的首地址 1000;
```

计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：

`a[i]_address = base_address + i * data_type_size`

data_type_size 表示数组中每个元素的大小，如果数组中存储的是整数型类型数据，那么 data_type_size 为 4 个字节。

**数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。**

数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。

假设数组的长度为 n，现在，我们需要将一个数据插入到一个数组中的第 K 个位置，为了把 K 个位置腾出来，给新的数据，我们需要将第 K~n 这部分的元素都顺序地往后挪一位。插入操作的最好时间复杂度为 O(1)，也就是直接插入到数组末尾，最坏时间复杂度为 O(n)，也就是在开头插入，平均时间复杂度 O(n);

如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必需按照刚才的方法搬移 k 之后的数据。但是如果数组中存储的数据并没有规律，数组只是被当成了一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 K 个位置，为了避免大规模的数据搬移，有一个简单粗暴的方法，就是直接把数据插入到数组末尾，把新的元素直接放到第 K 个位置上。

```js
const arr = ["a", "b", "c", "d", "e"];
// 把x插入到第三个位置
arr[2] = x;
arr.push("c");

// a b x d e c
```

将删除的数据标记为已删除，在数组没有足够的内存空间的时候，在触发真正的执行操作，这样就大大减少了删除操作导致的数据搬移。

以上就是 JVM 标记清除算法的核心思想；

数组越界，遍历一个数组 i <= arr.length 这个时候 i 会越界访问数组，计算机通过寻址公式开始查找内存地址，如果这个内存地址恰好是 i 的内存地址，那么 arr[arr.length] 想等于 i 同时赋值为 0，那么就会导致代码的无限循环。

并不是所有语言都像 C 一样，把越界检查交给程序员，Java 中越界访问会报错。

从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移(offset)”。

`a[k]_address = base_address + k * type_size`

// 换成 1 多了一次执行减法指令

`a[k]_address = base_address + (k - 1) * type_size`
